{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, make_scorer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RAW = '../data/raw_data/'\n",
    "PATH_INTER = '../data/intermediate/'\n",
    "PATH_DATA = '../data/data/'\n",
    "PATH_MODELS = '../../models/'\n",
    "PATH_IMAGES = '../assets/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(PATH_DATA + \"CRC_2880_train.csv\")\n",
    "val = pd.read_csv(PATH_DATA + \"CRC_320_val.csv\")\n",
    "test_CRC = pd.read_csv(PATH_DATA + \"CRC_800_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['read_score'] < 0.0, 'read_score'] = 0.0\n",
    "train.loc[train['read_score'] > 100.0, 'read_score'] = 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_counts(df, col):\n",
    "    \"\"\"Plot value counts of a column\"\"\"\n",
    "        \n",
    "    plt.figure(figsize = (12, 5))\n",
    "    df[col].value_counts().sort_index().plot.bar(color = 'blue',\n",
    "                                                 edgecolor = 'k',\n",
    "                                                 linewidth = 1.5)\n",
    "    plt.xlabel(f'{col}'); plt.title(f'{col} Value Counts'); plt.ylabel('Count')\n",
    "    plt.show();\n",
    "\n",
    "def plot_categoricals(x, y, data, annotate = True):\n",
    "    \"\"\"Plot counts of two categoricals.\n",
    "    Size is raw count for each grouping.\n",
    "    Percentages are for a given value of y.\"\"\"\n",
    "    \n",
    "    # Raw counts \n",
    "    raw_counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = False))\n",
    "    raw_counts = raw_counts.rename(columns = {x: 'raw_count'})\n",
    "    \n",
    "    # Calculate counts for each group of x and y\n",
    "    counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = True))\n",
    "    \n",
    "    # Rename the column and reset the index\n",
    "    counts = counts.rename(columns = {x: 'normalized_count'}).reset_index()\n",
    "    counts['percent'] = 100 * counts['normalized_count']\n",
    "    \n",
    "    # Add the raw count\n",
    "    counts['raw_count'] = list(raw_counts['raw_count'])\n",
    "    \n",
    "    plt.figure(figsize = (14, 10))\n",
    "    # Scatter plot sized by percent\n",
    "    plt.scatter(counts[x], counts[y], edgecolor = 'k', color = 'lightgreen',\n",
    "                s = 100 * np.sqrt(counts['raw_count']), marker = 'o',\n",
    "                alpha = 0.6, linewidth = 1.5)\n",
    "    \n",
    "    if annotate:\n",
    "        # Annotate the plot with text\n",
    "        for i, row in counts.iterrows():\n",
    "            # Put text with appropriate offsets\n",
    "            plt.annotate(xy = (row[x] - (1 / counts[x].nunique()), \n",
    "                               row[y] - (0.15 / counts[y].nunique())),\n",
    "                         color = 'navy',\n",
    "                         s = f\"{round(row['percent'], 1)}%\")\n",
    "        \n",
    "    # Set tick marks\n",
    "    plt.yticks(counts[y].unique())\n",
    "    plt.xticks(counts[x].unique())\n",
    "    \n",
    "    # Transform min and max to evenly space in square root domain\n",
    "    sqr_min = int(np.sqrt(raw_counts['raw_count'].min()))\n",
    "    sqr_max = int(np.sqrt(raw_counts['raw_count'].max()))\n",
    "    \n",
    "    # 5 sizes for legend\n",
    "    msizes = list(range(sqr_min, sqr_max,\n",
    "                        int(( sqr_max - sqr_min) / 5)))\n",
    "    markers = []\n",
    "    \n",
    "    # Markers for legend\n",
    "    for size in msizes:\n",
    "        markers.append(plt.scatter([], [], s = 100 * size, \n",
    "                                   label = f'{int(round(np.square(size) / 100) * 100)}', \n",
    "                                   color = 'lightgreen',\n",
    "                                   alpha = 0.6, edgecolor = 'k', linewidth = 1.5))\n",
    "        \n",
    "    # Legend and formatting\n",
    "    plt.legend(handles = markers, title = 'Counts',\n",
    "               labelspacing = 3, handletextpad = 2,\n",
    "               fontsize = 16,\n",
    "               loc = (1.10, 0.19))\n",
    "    \n",
    "    plt.annotate(f'* Size represents raw count while % is for a given y value.',\n",
    "                 xy = (0, 1), xycoords = 'figure points', size = 10)\n",
    "    \n",
    "    # Adjust axes limits\n",
    "    plt.xlim((counts[x].min() - (6 / counts[x].nunique()), \n",
    "              counts[x].max() + (6 / counts[x].nunique())))\n",
    "    plt.ylim((counts[y].min() - (4 / counts[y].nunique()), \n",
    "              counts[y].max() + (4 / counts[y].nunique())))\n",
    "    plt.grid(None)\n",
    "    plt.xlabel(f\"{x}\"); plt.ylabel(f\"{y}\"); plt.title(f\"{y} vs {x}\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag                1.000000\n",
       "bin_tag            0.866370\n",
       "num_positives      0.461502\n",
       "num_tokens         0.389669\n",
       "num_unk            0.382112\n",
       "num_char           0.381596\n",
       "sentiment          0.366212\n",
       "num_punct          0.357476\n",
       "num_discourse      0.345012\n",
       "num_modals         0.314870\n",
       "num_negatives      0.272517\n",
       "named_entities     0.178657\n",
       "num_upper          0.070443\n",
       "avg_word_length   -0.060761\n",
       "read_score        -0.290127\n",
       "Name: tag, dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correl = train.corr()\n",
    "\n",
    "correl['tag'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, y_bin_train = train.drop(['tag', 'bin_tag'], axis=1), train['tag'].copy(), train['bin_tag'].copy()\n",
    "X_val, y_val, y_bin_val = val.drop(['tag', 'bin_tag'], axis=1), val['tag'].copy(), val['bin_tag'].copy()\n",
    "X_test_CRC, y_test_CRC, y_bin_test_CRC = test_CRC.drop(['tag', 'bin_tag'], axis=1), test_CRC['tag'].copy(), test_CRC['bin_tag'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2880 entries, 0 to 2879\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   text_review      2880 non-null   object \n",
      " 1   read_score       2880 non-null   float64\n",
      " 2   sentiment        2880 non-null   float64\n",
      " 3   num_upper        2880 non-null   int64  \n",
      " 4   named_entities   2880 non-null   int64  \n",
      " 5   num_positives    2880 non-null   int64  \n",
      " 6   num_negatives    2880 non-null   int64  \n",
      " 7   num_unk          2880 non-null   int64  \n",
      " 8   num_punct        2880 non-null   int64  \n",
      " 9   num_discourse    2880 non-null   int64  \n",
      " 10  num_modals       2880 non-null   int64  \n",
      " 11  num_tokens       2880 non-null   int64  \n",
      " 12  num_char         2880 non-null   int64  \n",
      " 13  avg_word_length  2880 non-null   float64\n",
      " 14  text_pos         2880 non-null   object \n",
      " 15  lemmas           2880 non-null   object \n",
      "dtypes: float64(3), int64(10), object(3)\n",
      "memory usage: 360.1+ KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom scorer for cross validation\n",
    "scorer = make_scorer(f1_score, greater_is_better=True, average = 'weighted')\n",
    "\n",
    "numeric_features = list(X_train[['read_score', 'sentiment', 'num_tokens',\n",
    "                                 'num_char', 'num_upper', 'named_entities',\n",
    "                                 'num_positives', 'num_negatives', 'num_unk',\n",
    "                                 'num_punct', 'num_discourse', 'num_modals',\n",
    "                                 'avg_word_length']])\n",
    "\n",
    "text_features = list(X_train[['text_review', 'text_pos', 'lemmas']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = ColumnTransformer(transformers=[\n",
    "            ('review', TfidfVectorizer(min_df=1, max_df=0.1, encoding='utf-8', ngram_range=(1,2)), 'text_review'),\n",
    "            ('pos', TfidfVectorizer(encoding='utf-8', ngram_range=(1,2)), 'text_pos'),\n",
    "            ('lemma', TfidfVectorizer(min_df=1, max_df=0.1, encoding='utf-8', ngram_range=(1,2)), 'lemmas'),\n",
    "            ('num_attr', StandardScaler(), numeric_features),\n",
    "            ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "               ('union', trans),\n",
    "               ('clf', RandomForestClassifier(n_estimators=100, random_state=10, \n",
    "                               n_jobs = -1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(None, dtype=object) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-c47b7b1b35a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_final_estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0;32m--> 152\u001b[0;31m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# Dask dataframes may not return numeric shape[0] value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array array(None, dtype=object) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "X_train_trans = pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = cross_val_score(pipeline, X_train, y_train, cv = 10, scoring = scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
